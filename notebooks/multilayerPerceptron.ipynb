{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad175fc9-6a72-47b6-96f4-9add570a9d76",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d4fbd-8671-4dbb-8fc4-7d0ddf0f8c3e",
   "metadata": {},
   "source": [
    "Notation: The superscript $[i]$ represents the $i^{th}$ observation, while $(i)$ represents the $i^{th}$ layer.\n",
    "\n",
    "Assume a fully connected NN, i.e Multilayer Perceptrons.\n",
    "\n",
    "Training dataset: $\\mathcal{D}_{train}$\n",
    "\n",
    "Size of data: $N$ \n",
    "\n",
    "No. of input features: $d$\n",
    "\n",
    "No. of outputs: $q$\n",
    "\n",
    "Input matrix = $\\mathbf{x} \\in \\mathbb{R}^{d \\times 1}$\n",
    "\n",
    "No. of hidden units: $h$\n",
    "\n",
    "Hidden-layer weights: $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\n",
    "\n",
    "Hidden-layer biases: $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{h \\times 1}$\n",
    "\n",
    "Output of hidden units: $\\mathbf{h} \\in \\mathbb{R}^{h \\times 1}$\n",
    "\n",
    "Output-layer weights: $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$\n",
    "\n",
    "Output-layer biases: $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{q \\times 1}$\n",
    "\n",
    "A nonlinear-activation function: $\\sigma$\n",
    "\n",
    "And the output: $\\mathbf{o} \\in \\mathbb{R}^{q \\times 1}$\n",
    "\n",
    "Thus the mathematical representation of our model is\n",
    "\n",
    "\n",
    "$$\\mathbf{H} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})$$\n",
    "\n",
    "$$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h} + \\mathbf{b}^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061f13d-0d81-4f68-b9f5-29a529ed2c95",
   "metadata": {},
   "source": [
    "## Concrete Mathematical Representation\n",
    "\n",
    "\n",
    "$N = 4898$, $d=11$, $h = 22$\n",
    "\n",
    "`x` $\\leftarrow$ $\\mathbf{x} \\in \\mathbb{R}^{11 \\times 1}$\n",
    "\n",
    "`W1` $\\leftarrow$ $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{22 \\times 11}$\n",
    "\n",
    "`b1` $\\leftarrow$ $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{22 \\times 1}$\n",
    "\n",
    "`H` $\\leftarrow$ $\\mathbf{h} \\in \\mathbb{R}^{22 \\times 1}$\n",
    "\n",
    "`W2` $\\leftarrow$ $\\mathbf{w}^{(2)T} \\in \\mathbb{R}^{1 \\times 22}$\n",
    "\n",
    "`b2` $\\leftarrow$ $b^{(2)} \\in \\mathbb{R}^{1}$\n",
    "\n",
    "ReLU activation: $\\sigma(x) = \\max(x, 0)$\n",
    "\n",
    "`y_pred` $\\leftarrow$ $o \\in \\mathbb{R}^1$\n",
    "\n",
    "\n",
    "### Model's Equations\n",
    "\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})$$\n",
    "\n",
    "$$o = \\mathbf{w}^{(2)T} \\mathbf{h} + \\mathbf{b}^{(2)}$$\n",
    "\n",
    "\n",
    "### Loss function and Empirical Risk Function\n",
    "\n",
    "Let loss $L$, be $L = \\mathscr{l}(o, y)$\n",
    "\n",
    "`loss_fn` $\\leftarrow$ $l(o, y) = \\frac{1}{2} (y - o)^2$\n",
    "\n",
    "And the empirical risk $J = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathscr{c}(o^{[i]}, y^{[i]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b1f3d",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "\n",
    "#### Empirical Risk\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial L} = 1$$\n",
    "\n",
    "#### Loss\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial o} = -(y - o)$$\n",
    "\n",
    "#### Hidden to Output Layer\n",
    "\n",
    "##### Weights $\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}}\n",
    "=\n",
    "\\frac{\\partial J}{\\partial L}\n",
    "\\times\n",
    "\\frac{\\partial L}{\\partial o}\n",
    "\\times\n",
    "\\frac{\\partial o}{\\partial \\mathbf{w}^{(2)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}} \n",
    "= -(y-o)\n",
    "\\times\n",
    "\\frac{\\partial (\\mathbf{w}^{(2)T} \\mathbf{h})}{\\partial \\mathbf{w}^{(2)}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}} \n",
    "= -(y-o) \n",
    "\\mathbf{h}\n",
    "$$\n",
    "\n",
    "##### Biases $\\frac{\\partial J}{\\partial b^{(2)}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}}\n",
    "=\n",
    "\\frac{\\partial J}{\\partial L}\n",
    "\\times\n",
    "\\frac{\\partial L}{\\partial o}\n",
    "\\times\n",
    "\\frac{\\partial o}{\\partial \\mathbf{b}^{(2)}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}} = -(y-o)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441fb95",
   "metadata": {},
   "source": [
    "#### Input to Hidden Layer\n",
    "\n",
    "\n",
    "##### Weights $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$\n",
    "\n",
    "Let $\\mathbf{z}$ be the intermediate variable before for calculating elementwise activation of $\\sigma$, i.e. $\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "=\n",
    "\\frac{\\partial J}{\\partial L}\n",
    "\\times\n",
    "\\frac{\\partial L}{\\partial o}\n",
    "\\times\n",
    "\\frac{\\partial o}{\\partial \\mathbf{h}}\n",
    "\\times\n",
    "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\n",
    "\\times\n",
    "\\frac{\\partial (\\mathbf{W}^{(1)} \\mathbf{x})}{\\partial \\mathbf{W}^{(1)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "=\n",
    "\\left[\n",
    "\\left[\n",
    "- (y - o)\n",
    "\\mathbf{w}^{(2)}\n",
    "\\right]\n",
    "\\odot\n",
    "\\sigma^{\\prime}(z)\n",
    "\\right]\n",
    "\\mathbf{x}^{T}\n",
    "$$\n",
    "\n",
    "\n",
    "##### Biases $\\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}}\n",
    "=\n",
    "\\left[\n",
    "- (y - o)\n",
    "\\mathbf{w}^{(2)}\n",
    "\\right]\n",
    "\\odot\n",
    "\\sigma^{\\prime}(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519df211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# configuration\n",
    "TEST_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "class Utilities:\n",
    "    @staticmethod\n",
    "    def train_test_split(\n",
    "        dataset: np.ndarray, test_ratio: float, shuffle: bool = True\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(dataset)\n",
    "        n = dataset.shape[0]\n",
    "        n_test = int(test_ratio * n)\n",
    "        return dataset[:n_test, :], dataset[n_test:, :]\n",
    "\n",
    "    @staticmethod\n",
    "    def xy_split(\n",
    "        dataset: np.ndarray, output_last: bool = True\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        if output_last:\n",
    "            return dataset[:, :-1], dataset[:, -1:]\n",
    "        else:\n",
    "            return dataset[:, 1:], dataset[:, :1]\n",
    "\n",
    "    @staticmethod\n",
    "    def standard_scale(d_train: np.ndarray, d_test: np.ndarray) -> np.ndarray:\n",
    "        nfeat = d_train.shape[0]\n",
    "        d_train_mean = np.mean(d_train, axis=1).reshape((nfeat, 1))\n",
    "        d_train_std = np.std(d_train, axis=1).reshape((nfeat, 1))\n",
    "\n",
    "        d_train = np.abs(d_train - d_train_mean) / d_train_std\n",
    "        d_test = np.abs(d_test - d_train_mean) / d_train_std\n",
    "        return d_train, d_test, d_train_mean, d_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    def __init__(\n",
    "        self, n_features: int, hidden_size: int, learning_rate: float, activation: str\n",
    "    ) -> None:\n",
    "        self.w1, self.b1 = self.init_theta((hidden_size, n_features), (hidden_size, 1))\n",
    "        self.w2, self.b2 = self.init_theta((1, hidden_size), (1, 1))\n",
    "        self.lowest_risk = float(\"inf\")\n",
    "        self.eta = learning_rate\n",
    "\n",
    "        match activation:\n",
    "            case \"tanh\":\n",
    "                self._activation = self._tanh\n",
    "                self._activation_gradient = self._tanh_gradient\n",
    "            case _:\n",
    "                self._activation = self._relu\n",
    "                self._activation_gradient = self._relu_gradient\n",
    "\n",
    "    def _relu(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(0, xs)\n",
    "\n",
    "    def _relu_gradient(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return np.where(xs > 0, 1, 0)\n",
    "\n",
    "    def _tanh(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return np.tanh(xs)\n",
    "\n",
    "    def _tanh_gradient(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return 1 - np.multiply(self._tanh(xs), self._tanh(xs))\n",
    "\n",
    "    def _loss(self, y_trues: np.ndarray, y_preds: np.ndarray) -> np.ndarray:\n",
    "        return 0.5 * np.square(y_trues - y_preds)\n",
    "\n",
    "    def _loss_gradient(self, y_true: np.ndarray, y_preds: np.ndarray) -> np.ndarray:\n",
    "        return -(y_true - y_preds)\n",
    "\n",
    "    def _risk(self, y_trues: np.ndarray, y_preds: np.ndarray) -> float:\n",
    "        return float(np.mean(self._loss(y_trues, y_preds), axis=1))\n",
    "\n",
    "    def _theta2_gradient(self, y_true: np.ndarray, y_preds: np.ndarray, h: np.ndarray):\n",
    "        b2_gradient = self._loss_gradient(y_true, y_preds)\n",
    "        w2_gradient = (np.mean(b2_gradient, axis=1).reshape((1, 1)) * h).T\n",
    "        return w2_gradient, b2_gradient\n",
    "\n",
    "    def _theta1_gradient(\n",
    "        self, y_true: np.ndarray, y_preds: np.ndarray, xs: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        lhs = self._loss_gradient(y_true, y_preds) * self.w2.T\n",
    "        rhs = self._activation_gradient(self._layer1_forward(xs))\n",
    "        b1_gradient = np.multiply(lhs, rhs)\n",
    "        w1_gradient = b1_gradient @ xs.T\n",
    "\n",
    "        return w1_gradient, b1_gradient\n",
    "\n",
    "    def _layer1_activation(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return self._activation(self._layer1_forward(xs))\n",
    "\n",
    "    def _layer1_forward(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return (self.w1 @ xs) + self.b1\n",
    "\n",
    "    def _layer2_forward(self, h: np.ndarray) -> np.ndarray:\n",
    "        return (self.w2 @ h) + self.b2\n",
    "\n",
    "    def forward(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return self._layer2_forward(self._layer1_activation(xs))\n",
    "\n",
    "    def update_theta(self, xs: np.ndarray, y_trues: np.ndarray, y_preds: np.ndarray):\n",
    "        h = self._layer1_activation(xs)\n",
    "        dw1, db1 = self._theta1_gradient(y_trues, y_preds, xs)\n",
    "        dw2, db2 = self._theta2_gradient(y_trues, y_preds, h)\n",
    "\n",
    "        self.w1 -= self.eta * dw1\n",
    "        self.b1 -= self.eta * np.mean(db1, axis=1).reshape(self.b1.shape)\n",
    "\n",
    "        self.w2 -= self.eta * np.mean(dw2, axis=0).reshape(self.w2.shape)\n",
    "        self.b2 -= self.eta * np.mean(db2, axis=1).reshape(self.b2.shape)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        xs: np.ndarray,\n",
    "        ys: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "        eta: Optional[float] = None,\n",
    "    ):\n",
    "        n = xs.shape[1]\n",
    "\n",
    "        bw1, bb1 = self.w1, self.b1\n",
    "        bw2, bb2 = self.w2, self.b2\n",
    "        lowest_risk = self.lowest_risk\n",
    "\n",
    "        self.eta = eta or self.eta\n",
    "\n",
    "        for i in range(epochs):\n",
    "            total_epoch_risk = 0.0\n",
    "            updates = 0\n",
    "\n",
    "            start = 0\n",
    "            end = start + batch_size\n",
    "\n",
    "            while end <= n:\n",
    "                xb = xs[:, start:end]\n",
    "                yb = ys[:, start:end]\n",
    "                ob = self.forward(xb)\n",
    "                self.update_theta(xb, yb, ob)\n",
    "\n",
    "                total_epoch_risk += self._risk(yb, ob)\n",
    "                updates += 1\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "\n",
    "            risk = total_epoch_risk / updates\n",
    "            if risk < lowest_risk:\n",
    "                bw1, bb1 = self.w1, self.b1\n",
    "                bw2, bb2 = self.w2, self.b2\n",
    "                lowest_risk = risk\n",
    "            else:\n",
    "                self.w1, self.b1 = bw1, bb1\n",
    "                self.w2, self.b2 = bw2, bb2\n",
    "                risk = lowest_risk\n",
    "\n",
    "            # if i == 0 or (i + 1) % 100 == 0:\n",
    "            if (i + 1) % 5 == 1:\n",
    "                clear_output(wait=True)\n",
    "            print(f\"Epoch {i + 1} risk: {np.round(risk, 4)}\")\n",
    "\n",
    "        self.lowest_risk = lowest_risk\n",
    "\n",
    "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray):\n",
    "        o_test = self.forward(x_test)\n",
    "        print(f\"Test Empirical Risk: {np.round(self._risk(y_test, o_test), 4)}\")\n",
    "        return o_test\n",
    "\n",
    "    @staticmethod\n",
    "    def init_theta(\n",
    "        weights_shape: tuple[int, int], bias_shape: tuple[int, int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        w, b = map(lambda x: x**3, map(np.random.random, (weights_shape, bias_shape)))\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad080dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "white_wine_csv = \"../data/raw/winequality-white.csv\"\n",
    "red_wine_csv = \"../data/raw/winequality-red.csv\"\n",
    "\n",
    "white_wine = pd.read_csv(white_wine_csv, delimiter=\";\")\n",
    "white_wine_raw = white_wine.to_numpy()\n",
    "\n",
    "red_wine = pd.read_csv(red_wine_csv, delimiter=\";\")\n",
    "red_wine_raw = red_wine.to_numpy()\n",
    "\n",
    "test, train = Utilities.train_test_split(white_wine_raw, TEST_RATIO, shuffle=True)\n",
    "\n",
    "x_train, y_train = Utilities.xy_split(train)\n",
    "x_test, y_test = Utilities.xy_split(test)\n",
    "\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T\n",
    "\n",
    "x_train, x_test, x_mean, x_std = Utilities.standard_scale(x_train, x_test)\n",
    "y_train, y_test, y_mean, y_std = Utilities.standard_scale(y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68583242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(x_train.shape[0], 121, 0.0001, activation=\"relu\")\n",
    "model.train(x_train, y_train, epochs=50, batch_size=5, eta=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23398759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = 20\n",
    "# delta = 10\n",
    "# stop = start + delta\n",
    "# y_pred[0, start:stop], y_test[0, start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.evaluate(x_test, y_test)\n",
    "y_pred = (y_pred * y_std) + y_mean\n",
    "y_test_ = (y_test * y_std) + y_mean\n",
    "\n",
    "\n",
    "print(\"Distribution of True Values\", Counter(np.round(y_test_).tolist()[0]))\n",
    "print(\"Distribution of Predictions\", Counter(np.round(y_pred).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.evaluate(x_train, y_train)\n",
    "y_pred = (y_pred * y_std) + y_mean\n",
    "y_train_ = (y_train * y_std) + y_mean\n",
    "\n",
    "\n",
    "print(\"Distribution of True Values\", Counter(np.round(y_train_).tolist()[0]))\n",
    "print(\"Distribution of Predictions\", Counter(np.round(y_pred).tolist()[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
