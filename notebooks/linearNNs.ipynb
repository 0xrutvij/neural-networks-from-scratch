{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f0b3fe-30e1-40fc-b3d8-5eb7c76041ea",
   "metadata": {},
   "source": [
    "# Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a320b3d-02e1-4b4f-8e24-ee55fb7bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nnfs.losses import LossFunction\n",
    "from nnfs.optimizers import Optimizer\n",
    "from nnfs.utils import Preprocessing\n",
    "\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "# configuration\n",
    "TEST_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398b89b-3a53-4654-a606-2f051382b1a9",
   "metadata": {},
   "source": [
    "## Mathematical Representation\n",
    "\n",
    "Let $\\mathbf{x}^{[i]} = \\{ x^{[i]}_1, x^{[i]}_2, \\ldots x^{[i]}_{11}\\}$ represent the $i^{th}$ feature vector, where all $\\mathbf{x} \\in \\mathbb{R}^{11}$\n",
    "\n",
    "\n",
    "And $y^{[i]}$ represent the true value of the $i^{th}$ feature vector, where $y \\in \\{0, 2, \\ldots 10\\}$\n",
    "\n",
    "\n",
    "Given a training dataset $\\mathcal{D}_{train}$ with size $n$ in the form\n",
    "\n",
    "$$\\mathcal{D}_{train} = \\left\\{ [\\mathbf{x}^T, y]^{[1]}, \\ldots, [\\mathbf{x}^T, y]^{[n]} \\right\\}$$\n",
    "\n",
    "#### A loss function, \n",
    "\n",
    "$$c : \\mathbb{R}^{11} \\times \\mathbb{R} \\times \\mathbb{R}^{12} \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "#### An empirical risk function \n",
    "\n",
    "$$\\hat{\\mathscr{l}}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n}c(\\mathbf{x}^{[i]}, y^{[i]}, \\boldsymbol{\\theta})$$\n",
    "\n",
    "Where $\\boldsymbol{\\theta}$ is the parameter vector of the form $\\boldsymbol{\\theta} = [\\mathbf{w}^T, b]$\n",
    "\n",
    "#### Goal: to find a $\\boldsymbol{\\theta}^*$ which minimizes the empirical risk\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* = \\underset{\\boldsymbol{\\theta}}{argmin} \\;\\; \\hat{\\mathscr{l}}(\\boldsymbol{\\theta})$$\n",
    "\n",
    "\n",
    "#### Linear Neural Network\n",
    "\n",
    "\n",
    "$$\\hat{y} = w_1x_1 + \\ldots + w_{11}x_{11} + b$$\n",
    "\n",
    "\n",
    "$$\\hat{y}^{[i]} = \\mathbf{w}^T \\mathbf{x}^{[i]} + b$$\n",
    "\n",
    "Let our loss function be the *squared error* function, then\n",
    "\n",
    "$$c(\\mathbf{x}^{[i]}, y^{[i]}, \\boldsymbol{\\theta}) = \\frac{1}{2} \\left( \\hat{y}^{[i]} - y^{[i]} \\right)^2$$\n",
    "\n",
    "\n",
    "Thus the empirical risk function for this linear network is of the form\n",
    "\n",
    "$$\\hat{\\mathscr{l}}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( \\mathbf{w}^T \\mathbf{x}^{[i]} + b - y^{[i]} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5a1ce-57ba-46dc-853d-37445ccaae05",
   "metadata": {},
   "source": [
    "## Python Code\n",
    "\n",
    "\n",
    "- $\\mathcal{D}_{train}$ is `train`\n",
    "- $\\mathcal{D}_{test}$ is `test`\n",
    "- $\\boldsymbol{\\theta}$ is `theta`\n",
    "- $\\mathbf{w}$ is `weights`\n",
    "- $b$ is `bias`\n",
    "- $\\hat{\\mathscr{l}}$ is `risk_fn`\n",
    "- $\\mathbf{x}$ is `x`\n",
    "- $\\hat{y}$ is `y_pred`\n",
    "- $y$ is `y_true`\n",
    "- $c$ is `loss_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c81d9-65c0-4856-b9f3-c87dde813e2f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fdd61-45a0-4227-8ed3-a3381255d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wine_csv = \"../data/raw/winequality-white.csv\"\n",
    "red_wine_csv = \"../data/raw/winequality-red.csv\"\n",
    "\n",
    "white_wine = pd.read_csv(white_wine_csv, delimiter=\";\")\n",
    "white_wine_raw = white_wine.to_numpy()\n",
    "\n",
    "white_wine_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202c717",
   "metadata": {},
   "source": [
    "## Linear Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e437af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN:\n",
    "    def __init__(self, n_features: int, learning_rate: float) -> None:\n",
    "        self.weights, self.bias = self.init_theta((n_features, 1), (1, 1))\n",
    "        self.eta = learning_rate\n",
    "\n",
    "        self._loss = LossFunction.squared_error()\n",
    "        self._loss_gradient = self._loss.backward\n",
    "\n",
    "        self._optimizer = Optimizer.adam(learning_rate=learning_rate)\n",
    "\n",
    "    def forward(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return ((self.weights.T @ xs.T) + self.bias).T\n",
    "\n",
    "    def risk(self, y_trues: np.ndarray, y_preds: np.ndarray) -> float:\n",
    "        return float(np.mean(self._loss(y_trues, y_preds), axis=0))\n",
    "\n",
    "    def weight_gradient(\n",
    "        self, xs: np.ndarray, y_trues: np.ndarray, y_preds: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        return np.mean(xs * self._loss_gradient(y_trues, y_preds), axis=0)\n",
    "\n",
    "    def bias_gradient(self, y_trues: np.ndarray, y_preds: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(self._loss_gradient(y_trues, y_preds), axis=0)\n",
    "\n",
    "    def update_theta(self, xs: np.ndarray, y_trues: np.ndarray, y_preds: np.ndarray):\n",
    "        self.weights = self._optimizer.update(\n",
    "            self.weights,\n",
    "            self.weight_gradient(xs, y_trues, y_preds).reshape(self.weights.shape),\n",
    "        )\n",
    "\n",
    "        self.bias = self._optimizer.update(\n",
    "            self.bias, self.bias_gradient(y_trues, y_preds)\n",
    "        )\n",
    "\n",
    "    def train(self, xs: np.ndarray, ys: np.ndarray, rs: int = 10):\n",
    "        for i in range(rs):\n",
    "            y_preds = self.forward(xs)\n",
    "            if (i + 1) % (10 ** (np.log10(rs) - 1)) == 0:\n",
    "                print(f\"Epoch {i + 1} :: risk={round(self.risk(ys, y_preds), 4)}\")\n",
    "            self.update_theta(xs, ys, y_preds)\n",
    "\n",
    "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray):\n",
    "        y_pred = self.forward(x_test)\n",
    "        print(f\"R^2 Score: {self._loss.accuracy(y_test.T, y_pred.T)}\")\n",
    "        print(f\"Empirical Risk {round(self.risk(y_test, y_pred), 4)}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def init_theta(\n",
    "        weights_shape: tuple[int, int], bias_shape: tuple[int, int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        w, b = map(lambda x: x * 10, map(np.random.random, (weights_shape, bias_shape)))\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "true_ws = np.random.random(n) * 100\n",
    "true_b = 0\n",
    "xs = np.random.random((10000, n))\n",
    "\n",
    "true_ys = (true_ws.T @ xs.T).reshape((xs.shape[0], 1))\n",
    "\n",
    "test, train = Preprocessing.train_test_split(\n",
    "    np.concatenate((xs, true_ys), axis=1), TEST_RATIO, shuffle=True\n",
    ")\n",
    "x_train, y_train = Preprocessing.xy_split(train)\n",
    "x_test, y_test = Preprocessing.xy_split(test)\n",
    "\n",
    "x_train[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d80509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn=lambda y, yhat: 0.5 * np.sqrt(y - yhat)\n",
    "model = LinearNN(x_train.shape[1], learning_rate=1)\n",
    "\n",
    "\n",
    "model.train(x_train, y_train, rs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65810953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.weights.T.flatten(), model.bias.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(true_ws, true_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3f02f-c914-486a-85ff-71859572673c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
