{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad175fc9-6a72-47b6-96f4-9add570a9d76",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d4fbd-8671-4dbb-8fc4-7d0ddf0f8c3e",
   "metadata": {},
   "source": [
    "Notation: The superscript $[i]$ represents the $i^{th}$ observation, while $(i)$ represents the $i^{th}$ layer.\n",
    "\n",
    "Assume a fully connected NN, i.e Multilayer Perceptrons.\n",
    "\n",
    "Training dataset: $\\mathcal{D}_{train}$\n",
    "\n",
    "Size of data: $N$ \n",
    "\n",
    "No. of input features: $d$\n",
    "\n",
    "No. of outputs: $q$\n",
    "\n",
    "Input matrix = $\\mathbf{x} \\in \\mathbb{R}^{d \\times 1}$\n",
    "\n",
    "No. of hidden units: $h$\n",
    "\n",
    "Hidden-layer weights: $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$\n",
    "\n",
    "Hidden-layer biases: $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{h \\times 1}$\n",
    "\n",
    "Output of hidden units: $\\mathbf{h} \\in \\mathbb{R}^{h \\times 1}$\n",
    "\n",
    "Output-layer weights: $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$\n",
    "\n",
    "Output-layer biases: $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{q \\times 1}$\n",
    "\n",
    "A nonlinear-activation function: $\\sigma$\n",
    "\n",
    "And the output: $\\mathbf{o} \\in \\mathbb{R}^{q \\times 1}$\n",
    "\n",
    "Thus the mathematical representation of our model is\n",
    "\n",
    "\n",
    "$$\\mathbf{H} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})$$\n",
    "\n",
    "$$\\mathbf{o} = \\mathbf{W}^{(2)} \\mathbf{h} + \\mathbf{b}^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061f13d-0d81-4f68-b9f5-29a529ed2c95",
   "metadata": {},
   "source": [
    "## Concrete Mathematical Representation\n",
    "\n",
    "\n",
    "$N = 4898$, $d=11$, $h = 22$\n",
    "\n",
    "`x` $\\leftarrow$ $\\mathbf{x} \\in \\mathbb{R}^{11 \\times 1}$\n",
    "\n",
    "`W1` $\\leftarrow$ $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{22 \\times 11}$\n",
    "\n",
    "`b1` $\\leftarrow$ $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{22 \\times 1}$\n",
    "\n",
    "`H` $\\leftarrow$ $\\mathbf{h} \\in \\mathbb{R}^{22 \\times 1}$\n",
    "\n",
    "`W2` $\\leftarrow$ $\\mathbf{w}^{(2)T} \\in \\mathbb{R}^{1 \\times 22}$\n",
    "\n",
    "`b2` $\\leftarrow$ $b^{(2)} \\in \\mathbb{R}^{1}$\n",
    "\n",
    "ReLU activation: $\\sigma(x) = \\max(x, 0)$\n",
    "\n",
    "`y_pred` $\\leftarrow$ $o \\in \\mathbb{R}^1$\n",
    "\n",
    "\n",
    "### Model's Equations\n",
    "\n",
    "$$\\mathbf{h} = \\sigma(\\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)})$$\n",
    "\n",
    "$$o = \\mathbf{w}^{(2)T} \\mathbf{h} + \\mathbf{b}^{(2)}$$\n",
    "\n",
    "\n",
    "### Loss function and Empirical Risk Function\n",
    "\n",
    "Let loss $L$, be $L = \\mathscr{l}(o, y)$\n",
    "\n",
    "`loss_fn` $\\leftarrow$ $l(o, y) = \\frac{1}{2} (y - o)^2$\n",
    "\n",
    "And the empirical risk $J = \\frac{1}{n} \\sum_{i = 1}^{n} \\mathscr{c}(o^{[i]}, y^{[i]})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b1f3d",
   "metadata": {},
   "source": [
    "### Gradients\n",
    "\n",
    "\n",
    "#### Empirical Risk\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial L} = 1$$\n",
    "\n",
    "#### Loss\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial o} = -(y - o)$$\n",
    "\n",
    "#### Hidden to Output Layer\n",
    "\n",
    "##### Weights $\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}}\n",
    "=\n",
    "\\frac{\\partial J}{\\partial L}\n",
    "\\times\n",
    "\\frac{\\partial L}{\\partial o}\n",
    "\\times\n",
    "\\frac{\\partial o}{\\partial \\mathbf{w}^{(2)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}} \n",
    "= -(y-o)\n",
    "\\times\n",
    "\\frac{\\partial (\\mathbf{w}^{(2)T} \\mathbf{h})}{\\partial \\mathbf{w}^{(2)}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}^{(2)}} \n",
    "= -(y-o) \n",
    "\\mathbf{h}\n",
    "$$\n",
    "\n",
    "##### Biases $\\frac{\\partial J}{\\partial b^{(2)}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}}\n",
    "=\n",
    "\\frac{\\partial J}{\\partial L}\n",
    "\\times\n",
    "\\frac{\\partial L}{\\partial o}\n",
    "\\times\n",
    "\\frac{\\partial o}{\\partial \\mathbf{b}^{(2)}}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{b}^{(2)}} = -(y-o)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441fb95",
   "metadata": {},
   "source": [
    "#### Input to Hidden Layer\n",
    "\n",
    "\n",
    "##### Weights $\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}$\n",
    "\n",
    "Let $\\mathbf{z}$ be the intermediate variable before for calculating elementwise activation of $\\sigma$, i.e. $\\mathbf{z} = \\mathbf{W}^{(1)} \\mathbf{x} + \\mathbf{b}^{(1)}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "=\n",
    "\\frac{\\partial J}{\\partial L}\n",
    "\\times\n",
    "\\frac{\\partial L}{\\partial o}\n",
    "\\times\n",
    "\\frac{\\partial o}{\\partial \\mathbf{h}}\n",
    "\\times\n",
    "\\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\n",
    "\\times\n",
    "\\frac{\\partial (\\mathbf{W}^{(1)} \\mathbf{x})}{\\partial \\mathbf{W}^{(1)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "=\n",
    "\\left[\n",
    "\\left[\n",
    "- (y - o)\n",
    "\\mathbf{w}^{(2)}\n",
    "\\right]\n",
    "\\odot\n",
    "\\sigma^{\\prime}(z)\n",
    "\\right]\n",
    "\\mathbf{x}^{T}\n",
    "$$\n",
    "\n",
    "\n",
    "##### Biases $\\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{b}^{(1)}}\n",
    "=\n",
    "\\left[\n",
    "- (y - o)\n",
    "\\mathbf{w}^{(2)}\n",
    "\\right]\n",
    "\\odot\n",
    "\\sigma^{\\prime}(z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519df211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "from nnfs.activations import Activation\n",
    "from nnfs.losses import LossFunction\n",
    "from nnfs.optimizers import Optimizer\n",
    "from nnfs.utils import Preprocessing\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# configuration\n",
    "TEST_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        hidden_size: int,\n",
    "        act_fn: Activation,\n",
    "        loss_fn: LossFunction,\n",
    "        optimizer: Optimizer,\n",
    "    ) -> None:\n",
    "        self.w1, self.b1 = self.init_theta((hidden_size, n_features), (hidden_size, 1))\n",
    "        self.w2, self.b2 = self.init_theta((1, hidden_size), (1, 1))\n",
    "\n",
    "        self._activation = act_fn.forward\n",
    "        self._activation_gradient = act_fn.backward\n",
    "\n",
    "        self._loss = loss_fn\n",
    "        self._loss_gradient = loss_fn.backward\n",
    "        self.accuracy = loss_fn.accuracy\n",
    "\n",
    "        self._optimizer = optimizer\n",
    "\n",
    "    def _risk(self, y_trues: np.ndarray, y_preds: np.ndarray) -> float:\n",
    "        return float(np.mean(self._loss(y_trues, y_preds), axis=1))\n",
    "\n",
    "    def _theta2_gradient(self, y_true: np.ndarray, y_preds: np.ndarray, h: np.ndarray):\n",
    "        b2_gradient = self._loss_gradient(y_true, y_preds)\n",
    "        w2_gradient = (h * np.mean(b2_gradient, axis=-1, keepdims=True)).T\n",
    "        return w2_gradient, b2_gradient\n",
    "\n",
    "    def _theta1_gradient(\n",
    "        self, y_true: np.ndarray, y_preds: np.ndarray, xs: np.ndarray\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        lhs = self._loss_gradient(y_true, y_preds) * self.w2.T\n",
    "        rhs = self._activation_gradient(self._layer1_forward(xs))\n",
    "        b1_gradient = np.multiply(lhs, rhs)\n",
    "        w1_gradient = b1_gradient @ xs.T\n",
    "\n",
    "        return w1_gradient, b1_gradient\n",
    "\n",
    "    def _layer1_activation(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return self._activation(self._layer1_forward(xs))\n",
    "\n",
    "    def _layer1_forward(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return (self.w1 @ xs) + self.b1\n",
    "\n",
    "    def _layer2_forward(self, h: np.ndarray) -> np.ndarray:\n",
    "        return (self.w2 @ h) + self.b2\n",
    "\n",
    "    def forward(self, xs: np.ndarray) -> np.ndarray:\n",
    "        return self._layer2_forward(self._layer1_activation(xs))\n",
    "\n",
    "    def update_theta(self, xs: np.ndarray, y_trues: np.ndarray, y_preds: np.ndarray):\n",
    "        h = self._layer1_activation(xs)\n",
    "        dw1, db1 = self._theta1_gradient(y_trues, y_preds, xs)\n",
    "        # print(np.abs(dw1))\n",
    "        dw2, db2 = self._theta2_gradient(y_trues, y_preds, h)\n",
    "\n",
    "        self.w1 = self._optimizer.update(self.w1, dw1)\n",
    "        self.b1 = self._optimizer.update(\n",
    "            self.b1, np.mean(db1, axis=1).reshape(self.b1.shape)\n",
    "        )\n",
    "\n",
    "        self.w2 = self._optimizer.update(\n",
    "            self.w2, np.mean(dw2, axis=0).reshape(self.w2.shape)\n",
    "        )\n",
    "\n",
    "        self.b2 = self._optimizer.update(\n",
    "            self.b2, np.mean(db2, axis=1).reshape(self.b2.shape)\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        xs: np.ndarray,\n",
    "        ys: np.ndarray,\n",
    "        epochs: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        n = xs.shape[1]\n",
    "\n",
    "        for i in range(epochs):\n",
    "            total_epoch_risk = 0.0\n",
    "            updates = 0\n",
    "\n",
    "            start = 0\n",
    "            end = start + batch_size\n",
    "\n",
    "            while end <= n:\n",
    "                xb = xs[:, start:end]\n",
    "                yb = ys[:, start:end]\n",
    "                ob = self.forward(xb)\n",
    "                self.update_theta(xb, yb, ob)\n",
    "\n",
    "                total_epoch_risk += self._risk(yb, ob)\n",
    "                updates += 1\n",
    "                start += batch_size\n",
    "                end += batch_size\n",
    "\n",
    "            if (i + 1) % 5 == 1:\n",
    "                clear_output(wait=True)\n",
    "            print(f\"Epoch {i + 1} risk: {np.round(total_epoch_risk / updates, 4)}\")\n",
    "\n",
    "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray):\n",
    "        o_test = self.forward(x_test)\n",
    "        print(f\"R^2 Value: {self.accuracy(y_test.T, o_test.T)}\")\n",
    "        print(f\"Empirical Risk: {np.round(self._risk(y_test, o_test), 4)}\")\n",
    "        return o_test\n",
    "\n",
    "    @staticmethod\n",
    "    def init_theta(\n",
    "        weights_shape: tuple[int, int], bias_shape: tuple[int, int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        w, b = map(\n",
    "            lambda x: (x - 0.5) / 100,\n",
    "            map(np.random.random, (weights_shape, bias_shape)),\n",
    "        )\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad080dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "white_wine_csv = \"../data/raw/winequality-white.csv\"\n",
    "red_wine_csv = \"../data/raw/winequality-red.csv\"\n",
    "\n",
    "white_wine = pd.read_csv(white_wine_csv, delimiter=\";\")\n",
    "\n",
    "display(white_wine)\n",
    "display(white_wine.describe())\n",
    "white_wine_raw = white_wine.to_numpy()\n",
    "\n",
    "red_wine = pd.read_csv(red_wine_csv, delimiter=\";\")\n",
    "red_wine_raw = red_wine.to_numpy()\n",
    "\n",
    "wines_raw = np.concatenate((white_wine_raw, red_wine_raw))\n",
    "# red_wine_raw\n",
    "\n",
    "wine_mean: np.ndarray = np.mean(wines_raw, axis=0, keepdims=True)\n",
    "wines_std: np.ndarray = np.std(wines_raw, axis=0, keepdims=True)\n",
    "\n",
    "wines_raw = (wines_raw - wine_mean) / wines_std\n",
    "\n",
    "test, train = Preprocessing.train_test_split(wines_raw, TEST_RATIO, shuffle=True)\n",
    "\n",
    "x_train, y_train = Preprocessing.xy_split(train)\n",
    "x_test, y_test = Preprocessing.xy_split(test)\n",
    "\n",
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T\n",
    "\n",
    "wine_mean, wines_std = wine_mean.flatten(), wines_std.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68583242",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(\n",
    "    n_features=x_train.shape[0],\n",
    "    hidden_size=100,\n",
    "    act_fn=Activation.sigmoid(),\n",
    "    loss_fn=LossFunction.squared_error(),\n",
    "    optimizer=Optimizer.adam(learning_rate=1e-3),\n",
    ")\n",
    "model.train(x_train, y_train, epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec9959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.evaluate(x_test, y_test)\n",
    "y_pred_: np.ndarray = (y_pred * wines_std[-1]) + wine_mean[-1]\n",
    "y_test_: np.ndarray = (y_test * wines_std[-1]) + wine_mean[-1]\n",
    "\n",
    "\n",
    "print(\"Distribution of True Values\", Counter(np.round(y_test_).tolist()[0]))\n",
    "print(\"Distribution of Predictions\", Counter(np.round(y_pred_).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.evaluate(x_train, y_train)\n",
    "y_pred2: np.ndarray = (y_pred * wines_std[-1]) + wine_mean[-1]\n",
    "y_train_: np.ndarray = (y_train * wines_std[-1]) + wine_mean[-1]\n",
    "\n",
    "\n",
    "print(\"Distribution of True Values\", Counter(np.round(y_train_.flatten()).tolist()))\n",
    "print(\"Distribution of Predictions\", Counter(np.round(y_pred2.flatten()).tolist()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7b407a09",
   "metadata": {},
   "source": [
    "Log transform for normalizing the input dataset (TODO)\n",
    "\n",
    "For the input patterns, subtract mean, dot product of transpose, || *like cosine similarity?* (TODO)\n",
    "\n",
    "Remove confusable patterns from the dataset.  (TODO)\n",
    "\n",
    "**Prediction on the extremes [1, 10] then [1, 2, 9, 10]** (DONE)\n",
    "\n",
    "Pick input vectors as weight initializations. (TODO)\n",
    "\n",
    "RBF activation or Sigmoid xform (TODO)\n",
    "\n",
    "Compute the eigen vectors of the inputs, and use them as weight inits. <- define the basis set, (TODO)\n",
    "\n",
    "Or even the covariance matrix. (TODO)\n",
    "\n",
    "Make the features categorical (TODO)\n",
    "\n",
    "Subset the wine dataset to create a toy problem (DONE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
