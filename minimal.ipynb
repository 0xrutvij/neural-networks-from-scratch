{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2f0b3fe-30e1-40fc-b3d8-5eb7c76041ea",
   "metadata": {},
   "source": [
    "# Linear Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a320b3d-02e1-4b4f-8e24-ee55fb7bb10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "\n",
    "# configuration\n",
    "TEST_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398b89b-3a53-4654-a606-2f051382b1a9",
   "metadata": {},
   "source": [
    "## Mathematical Representation\n",
    "\n",
    "Let $\\mathbf{x}^{[i]} = \\{ x^{[i]}_1, x^{[i]}_2, \\ldots x^{[i]}_{11}\\}$ represent the $i^{th}$ feature vector, where all $\\mathbf{x} \\in \\mathbb{R}^{11}$\n",
    "\n",
    "\n",
    "And $y^{[i]}$ represent the true value of the $i^{th}$ feature vector, where $y \\in \\{0, 2, \\ldots 10\\}$\n",
    "\n",
    "\n",
    "Given a training dataset $\\mathcal{D}_{train}$ with size $n$ in the form\n",
    "\n",
    "$$\\mathcal{D}_{train} = \\left\\{ [\\mathbf{x}^T, y]^{[1]}, \\ldots, [\\mathbf{x}^T, y]^{[n]} \\right\\}$$\n",
    "\n",
    "#### A loss function, \n",
    "\n",
    "$$c : \\mathbb{R}^{11} \\times \\mathbb{R} \\times \\mathbb{R}^{12} \\mapsto \\mathbb{R}$$\n",
    "\n",
    "#### An empirical risk function \n",
    "\n",
    "$$\\hat{\\mathscr{l}}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n}c(\\mathbf{x}^{[i]}, y^{[i]}, \\boldsymbol{\\theta})$$\n",
    "\n",
    "Where $\\boldsymbol{\\theta}$ is the parameter vector of the form $\\boldsymbol{\\theta} = [\\mathbf{w}^T, b]$\n",
    "\n",
    "#### Goal: to find a $\\boldsymbol{\\theta}^*$ which minimizes the empirical risk\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* = \\underset{\\boldsymbol{\\theta}}{argmin} \\;\\; \\hat{\\mathscr{l}}(\\boldsymbol{\\theta})$$\n",
    "\n",
    "\n",
    "#### Linear Neural Network\n",
    "\n",
    "\n",
    "$$\\hat{y} = w_1x_1 + \\ldots + w_{11}x_{11} + b$$\n",
    "\n",
    "\n",
    "$$\\hat{y}^{[i]} = \\mathbf{w}^T \\mathbf{x}^{[i]} + b$$\n",
    "\n",
    "Let our loss function be the *squared error* function, then\n",
    "\n",
    "$$c(\\mathbf{x}^{[i]}, y^{[i]}, \\boldsymbol{\\theta}) = \\frac{1}{2} \\left( \\hat{y}^{[i]} - y^{[i]} \\right)^2$$\n",
    "\n",
    "\n",
    "Thus the empirical risk function for this linear network is of the form\n",
    "\n",
    "$$\\hat{\\mathscr{l}}(\\boldsymbol{\\theta}) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{2} \\left( \\mathbf{w}^T \\mathbf{x}^{[i]} + b - y^{[i]} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5a1ce-57ba-46dc-853d-37445ccaae05",
   "metadata": {},
   "source": [
    "## Python Code\n",
    "\n",
    "\n",
    "- $\\mathcal{D}_{train}$ is `train`\n",
    "- $\\mathcal{D}_{test}$ is `test`\n",
    "- $\\boldsymbol{\\theta}$ is `theta`\n",
    "- $\\mathbf{w}$ is `weights`\n",
    "- $b$ is `bias`\n",
    "- $\\hat{\\mathscr{l}}$ is `risk_fn`\n",
    "- $\\mathbf{x}$ is `x`\n",
    "- $\\hat{y}$ is `y_pred`\n",
    "- $y$ is `y_true`\n",
    "- $c$ is `loss_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599dec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "class Utilities:\n",
    "    @staticmethod\n",
    "    def train_test_split(\n",
    "        dataset: np.ndarray, test_ratio: float, shuffle: bool = True\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "        if shuffle:\n",
    "            np.random.shuffle(dataset)\n",
    "        n = dataset.shape[0]\n",
    "        n_test = int(test_ratio * n)\n",
    "        return dataset[:n_test, :], dataset[n_test:, :]\n",
    "\n",
    "    @staticmethod\n",
    "    def xy_split(\n",
    "        dataset: np.ndarray, output_last: bool = True\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        if output_last:\n",
    "            return dataset[:, :-1], dataset[:, -1:]\n",
    "        else:\n",
    "            return dataset[:, 1:], dataset[:, :1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c81d9-65c0-4856-b9f3-c87dde813e2f",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fdd61-45a0-4227-8ed3-a3381255d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_wine_csv = \"data/raw/winequality-white.csv\"\n",
    "red_wine_csv = \"data/raw/winequality-red.csv\"\n",
    "\n",
    "white_wine = pd.read_csv(white_wine_csv, delimiter=\";\")\n",
    "white_wine_raw = white_wine.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202c717",
   "metadata": {},
   "source": [
    "## Linear Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e437af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNN:\n",
    "    def __init__(self, n_features: int, learning_rate: float) -> None:\n",
    "        self.weights, self.bias = self.init_theta((n_features, 1), (1, 1))\n",
    "        self.eta = learning_rate\n",
    "\n",
    "    def forward(self, xs: np.ndarray) -> float:\n",
    "        return np.double((self.weights.T @ xs.T) + self.bias).T\n",
    "\n",
    "    def losses(self, y_trues: np.ndarray, y_preds: np.ndarray) -> np.ndarray:\n",
    "        return np.square(y_trues - y_preds) * 0.5\n",
    "\n",
    "    def risk(self, y_trues: np.ndarray, y_preds: np.ndarray) -> float:\n",
    "        return np.mean(self.losses(y_trues, y_preds))\n",
    "\n",
    "    def weight_gradient(\n",
    "        self, xs: np.ndarray, y_trues: np.ndarray, y_preds: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        return np.mean(xs * -(y_trues - y_preds), axis=0)\n",
    "\n",
    "    def bias_gradient(self, y_trues: np.ndarray, y_preds: np.ndarray) -> np.ndarray:\n",
    "        return np.double(np.mean(-(y_trues - y_preds)))\n",
    "\n",
    "    def update_theta(self, xs: np.ndarray, y_trues: np.ndarray, y_preds: np.ndarray):\n",
    "        self.weights -= self.eta * self.weight_gradient(xs, y_trues, y_preds).reshape(\n",
    "            self.weights.shape\n",
    "        )\n",
    "        self.bias -= self.eta * self.bias_gradient(y_trues, y_preds)\n",
    "\n",
    "    def train(self, xs: np.ndarray, ys: np.ndarray, rs: int = 10):\n",
    "        for i in range(rs):\n",
    "            y_preds = self.forward(xs)\n",
    "            if (i + 1) % (10 ** (np.log10(rs) - 1)) == 0:\n",
    "                print(f\"Epoch {i + 1} :: risk={self.risk(ys, y_preds)}\")\n",
    "            self.update_theta(xs, ys, y_preds)\n",
    "\n",
    "    def evaluate(self, x_test: np.ndarray, y_test: np.ndarray):\n",
    "        print(f\"Empirical Risk {self.risk(y_test, self.forward(x_test))}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def init_theta(\n",
    "        weights_shape: tuple[int, int], bias_shape: tuple[int, int]\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        w, b = map(lambda x: x * 10, map(np.random.random, (weights_shape, bias_shape)))\n",
    "        return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "true_ws = np.random.random(n) * 100\n",
    "true_b = 0\n",
    "xs = np.random.random((10000, n))\n",
    "\n",
    "true_ys = (true_ws.T @ xs.T).reshape((xs.shape[0], 1))\n",
    "\n",
    "test, train = Utilities.train_test_split(\n",
    "    np.concatenate((xs, true_ys), axis=1), TEST_RATIO, shuffle=True\n",
    ")\n",
    "x_train, y_train = Utilities.xy_split(train)\n",
    "x_test, y_test = Utilities.xy_split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d80509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn=lambda y, yhat: 0.5 * np.sqrt(y - yhat)\n",
    "model = LinearNN(x_train.shape[1], learning_rate=1)\n",
    "\n",
    "\n",
    "model.train(x_train, y_train, rs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65810953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights.T, model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec51874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ws, true_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808cfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnfs",
   "language": "python",
   "name": "nnfs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
